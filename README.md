# Daily ArXiv HCI

A curated collection of arXiv papers with open-source implementations, specifically focusing on Human-Computer Interaction (cs.HC) and related fields like Computer Graphics (cs.GR), Computer Vision (cs.CV), etc. This repository aims to serve researchers and practitioners in HCI by providing easy access to papers that come with their source code implementations.

## Overview
This project automatically tracks and analyzes papers from relevant HCI categories on arXiv daily using GitHub Actions. It specifically identifies and catalogs papers that have released their source code, making it easier for researchers in HCI and related areas to find implementable research work.

The main features include:
- Daily updates of papers with open-source implementations
- Focus on Human-Computer Interaction and related research
- Automatic tracking and organization

## Latest Updates 
|date|paper|code|
|---|---|---|
|2505.04119|[gaprompt: geometry-aware point cloud prompt for 3d vision model](https://arxiv.org/abs/2505.04119)|[icml2025-vgp](https://github.com/zhoujiahuan1991/icml2025-vgp)|
|2505.04121|[vision graph prompting via semantic low-rank decomposition](https://arxiv.org/abs/2505.04121)|[icml2025-vgp](https://github.com/zhoujiahuan1991/icml2025-vgp)|
|2505.08614|[waveguard: robust deepfake detection and source tracing via dual-tree complex wavelet and graph neural networks](https://arxiv.org/abs/2505.08614)|[waveguard](https://github.com/vpsg-research/waveguard)|
|2505.11131|[one image is worth a thousand words: a usability preservable text-image collaborative erasing framework](https://arxiv.org/abs/2505.11131)|[co-erasing](https://github.com/ferry-li/co-erasing)|
|2505.12266|[pmq-ve: progressive multi-frame quantization for video enhancement](https://arxiv.org/abs/2505.12266)|[pmq-ve](https://github.com/xiaobigfeng/pmq-ve)|
|2505.12513|[globalgeotree: a multi-granular vision-language dataset for global tree species classification](https://arxiv.org/abs/2505.12513)|[globalgeotree](https://github.com/muyang99/globalgeotree)|
|2505.12728|[flash: latent-aware semi-autoregressive speculative decoding for multimodal tasks](https://arxiv.org/abs/2505.12728)|[flashsd](https://github.com/zihuaevan/flashsd)|
|2505.13061|[3d visual illusion depth estimation](https://arxiv.org/abs/2505.13061)|[3d-visual-illusion-depth-estimation](https://github.com/yaochengtang/3d-visual-illusion-depth-estimation)|
|2505.13740|[improving compositional generation with diffusion models using lift scores](https://arxiv.org/abs/2505.13740)|[complift](https://github.com/rainorangelemon/complift)|
|2505.14362|[deepeyes: incentivizing "thinking with images" via reinforcement learning](https://arxiv.org/abs/2505.14362)|[deepeyes](https://github.com/visual-agent/deepeyes)|
|2505.15235|[x-grm: large gaussian reconstruction model for sparse-view x-rays to computed tomography](https://arxiv.org/abs/2505.15235)|[x-grm](https://github.com/cuhk-aim-group/x-grm)|
|2505.15660|[exploring the limits of vision-language-action manipulations in cross-task generalization](https://arxiv.org/abs/2505.15660)|[X-ICM](https://github.com/jiaming-zhou/X-ICM)|
|2505.16091|[oscar: one-step diffusion codec across multiple bit-rates](https://arxiv.org/abs/2505.16091)|[oscar](https://github.com/jp-guo/oscar)|
|2505.16882|[tracking the flight: exploring a computational framework for analyzing escape responses in plains zebra (equus quagga)](https://arxiv.org/abs/2505.16882)|[zebras-stitching](https://github.com/neuroinformatics-unit/zebras-stitching)|
|2505.16990|[dimple: discrete diffusion multimodal large language model with parallel decoding](https://arxiv.org/abs/2505.16990)|[dimple](https://github.com/yu-rp/dimple)|
|2505.18156|[injectlab: a tactical framework for adversarial threat modeling against large language models](https://arxiv.org/abs/2505.18156)|[injectlab](https://github.com/ahow2004/injectlab)|
|2505.18175|[evaluation in eeg emotion recognition: state-of-the-art review and unified framework](https://arxiv.org/abs/2505.18175)|[eegain](https://github.com/emotionlab/eegain)|
|2505.18197|[a novel benchmark and dataset for efficient 3d gaussian splatting with gaussian point cloud compression](https://arxiv.org/abs/2505.18197)|[GausPcc](https://github.com/Wangkkklll/GausPcc)|
|2505.18412|[rehabilitation exercise quality assessment and feedback generation using large language models with prompt engineering](https://arxiv.org/abs/2505.18412)|[exercisellm](https://github.com/jessicaxtang/exercisellm)|
|2505.18424|[how we won the isles'24 challenge by preprocessing](https://arxiv.org/abs/2505.18424)|[ISLES2024](https://github.com/KurtLabUW/ISLES2024)|
|2505.18487|[grounding bodily awareness in visual representations for efficient policy learning](https://arxiv.org/abs/2505.18487)|[icon](https://github.com/henrywjl/icon)|
|2505.18536|[reinforcement fine-tuning powers reasoning capability of multimodal large language models](https://arxiv.org/abs/2505.18536)|[awesome-rl-based-reasoning-mllms](https://github.com/sun-haoyuan23/awesome-rl-based-reasoning-mllms)|
|2505.18546|[reflectgan: modeling vegetation effects for soil carbon estimation from satellite imagery](https://arxiv.org/abs/2505.18546)|[reflectgan](https://github.com/dristidatta/reflectgan)|
|2505.18547|[diffusion blend: inference-time multi-preference alignment for diffusion models](https://arxiv.org/abs/2505.18547)|[db-2025](https://github.com/bluewoods127/db-2025)|
|2505.18730|[align beyond prompts: evaluating world knowledge alignment in text-to-image generation](https://arxiv.org/abs/2505.18730)|[abp](https://github.com/smile365317/abp)|
|2505.18787|[think twice before adaptation: improving adaptability of deepfake detection via online test-time adaptation](https://arxiv.org/abs/2505.18787)|[t2a-think-twice-before-adaptation](https://github.com/honghanh2104/t2a-think-twice-before-adaptation)|
|2505.18829|[litecua: computer as mcp server for computer-use agent on aios](https://arxiv.org/abs/2505.18829)|[aios](https://github.com/agiresearch/aios)|
|2505.18956|[how do images align and complement lidar? towards a harmonized multi-modal 3d panoptic segmentation](https://arxiv.org/abs/2505.18956)|[ial](https://github.com/impl-lab/ial)|
|2505.18983|[amorlip: efficient language-image pretraining via amortization](https://arxiv.org/abs/2505.18983)|[amorlip](https://github.com/haotiansun14/amorlip)|
|2505.18985|[strict: stress test of rendering images containing text](https://arxiv.org/abs/2505.18985)|[strict-bench](https://github.com/tianyu-z/strict-bench)|
|2505.18989|[spars: self-play adversarial reinforcement learning for segmentation of liver tumours](https://arxiv.org/abs/2505.18989)|[spars](https://github.com/catalinatan/spars)|
|2505.19000|[veripo: cultivating long reasoning in video-llms via verifier-gudied iterative policy optimization](https://arxiv.org/abs/2505.19000)|[veripo](https://github.com/hitsz-tmg/veripo)|
|2505.19015|[can multimodal large language models understand spatial relations?](https://arxiv.org/abs/2505.19015)|[spatialmqa](https://github.com/ziyan-xiaoyu/spatialmqa)|
|2505.19028|[infochartqa: a benchmark for multimodal question answering on infographic charts](https://arxiv.org/abs/2505.19028)|[infochartqa](https://github.com/cooldawnant/infochartqa)|
|2505.19031|[medical large vision language models with multi-image visual ability](https://arxiv.org/abs/2505.19031)|[med-mim](https://github.com/xikai97/med-mim)|
|2505.19065|[mmp-2k: a benchmark multi-labeled macro photography image quality assessment database](https://arxiv.org/abs/2505.19065)|[mmp-2k](https://github.com/future-iqa/mmp-2k)|
|2505.19084|[jodi: unification of visual generation and understanding via joint modeling](https://arxiv.org/abs/2505.19084)|[jodi](https://github.com/vipl-genun/jodi)|
|2505.19094|[satori-r1: incentivizing multimodal reasoning with spatial grounding and verifiable rewards](https://arxiv.org/abs/2505.19094)|[satori-r1](https://github.com/justairr/satori-r1)|
|2505.19120|[freqformer: image-demoir\'eing transformer via efficient frequency decomposition](https://arxiv.org/abs/2505.19120)|[freqformer](https://github.com/xyliu339/freqformer)|
|2505.19147|[shifting ai efficiency from model-centric to data-centric compression](https://arxiv.org/abs/2505.19147)|[awesome-token-level-model-compression](https://github.com/xuyang-liu16/awesome-token-level-model-compression)|
|2505.19148|[dista-net: dynamic closely-spaced infrared small target unmixing](https://arxiv.org/abs/2505.19148)|[grokcso](https://github.com/grokcv/grokcso)|
|2505.19159|[a joint learning framework with feature reconstruction and prediction for incomplete satellite image time series in agricultural semantic segmentation](https://arxiv.org/abs/2505.19159)|[joint_frp](https://github.com/wangyuze-csu/joint_frp)|
|2505.19161|[benchmarking laparoscopic surgical image restoration and beyond](https://arxiv.org/abs/2505.19161)|[surgical-image-restoration](https://github.com/pjlallen/surgical-image-restoration)|
|2505.19190|[i2moe: interpretable multimodal interaction-aware mixture-of-experts](https://arxiv.org/abs/2505.19190)|[i2moe](https://github.com/raina-xin/i2moe)|
|2505.19196|[step-level reward for free in rl-based t2i diffusion model fine-tuning](https://arxiv.org/abs/2505.19196)|[coca](https://github.com/lil-shake/coca)|
|2505.19218|[advancing video self-supervised learning via image foundation models](https://arxiv.org/abs/2505.19218)|[advise-video-ssl](https://github.com/jingwwu/advise-video-ssl)|
|2505.19225|[meditok: a unified tokenizer for medical image synthesis and interpretation](https://arxiv.org/abs/2505.19225)|[meditok](https://github.com/masaaki-75/meditok)|
|2505.19235|[corematching: a co-adaptive sparse inference framework with token and neuron pruning for comprehensive acceleration of vision-language models](https://arxiv.org/abs/2505.19235)|[2025-icml-corematching](https://github.com/wangqinsi1/2025-icml-corematching)|
|2505.19249|[rgc-bent: a novel dataset for bent radio galaxy classification](https://arxiv.org/abs/2505.19249)|[rgc-bent](https://github.com/mirsazzathossain/rgc-bent)|
|2505.19256|[polypose: localizing deformable anatomy in 3d from sparse 2d x-ray images using polyrigid transforms](https://arxiv.org/abs/2505.19256)|[polypose](https://github.com/eigenvivek/polypose)|
|2505.19264|[improving novel view synthesis of 360$^\circ$ scenes in extremely sparse views by jointly training hemisphere sampled synthetic images](https://arxiv.org/abs/2505.19264)|[hemisparsegs](https://github.com/angchen-dev/hemisparsegs)|
|2505.19319|[holistic white-light polyp classification via alignment-free dense distillation of auxiliary optical chromoendoscopy](https://arxiv.org/abs/2505.19319)|[add](https://github.com/huster-hq/add)|
|2505.19328|[bah dataset for ambivalence/hesitancy recognition in videos for behavioural change](https://arxiv.org/abs/2505.19328)|[bah-dataset](https://github.com/sbelharbi/bah-dataset)|
|2505.19434|[cstrack: enhancing rgb-x tracking via compact spatiotemporal features](https://arxiv.org/abs/2505.19434)|[cstrack](https://github.com/xiaokunfeng/cstrack)|
|2505.19455|[mm-prompt: cross-modal prompt tuning for continual visual question answering](https://arxiv.org/abs/2505.19455)|[cvqa](https://github.com/xli04/cvqa)|
|2505.19503|[locality-aware zero-shot human-object interaction detection](https://arxiv.org/abs/2505.19503)|[lain](https://github.com/oreochocolate/lain)|
|2505.19536|[flowcut: rethinking redundancy via information flow for efficient vision-language models](https://arxiv.org/abs/2505.19536)|[flowcut](https://github.com/tungchintao/flowcut)|
|2505.19564|[k-buffers: a plug-in method for enhancing neural fields with multiple buffers](https://arxiv.org/abs/2505.19564)|[k-buffers](https://github.com/renhaofan/k-buffers)|
|2505.19571|[vtbench: comprehensive benchmark suite towards real-world virtual try-on models](https://arxiv.org/abs/2505.19571)|[vtbench](https://github.com/huuxiaobin/vtbench)|
|2505.19611|[align and surpass human camouflaged perception: visual refocus reinforcement fine-tuning](https://arxiv.org/abs/2505.19611)|[vrrf](https://github.com/huuxiaobin/vrrf)|
|2505.19618|[rotation-equivariant self-supervised method in image denoising](https://arxiv.org/abs/2505.19618)|[adarenet](https://github.com/liuhanze623/adarenet)|
|2505.19638|[hf-vton: high-fidelity virtual try-on via consistent geometric and semantic alignment](https://arxiv.org/abs/2505.19638)|[hf-vton](https://github.com/mmlph/hf-vton)|
|2505.19652|[sacm: seeg-audio contrastive matching for chinese speech decoding](https://arxiv.org/abs/2505.19652)|[SACM](https://github.com/WangHongbinary/SACM)|
|2505.19659|[langdaug: langevin data augmentation for multi-source domain generalization in medical image segmentation](https://arxiv.org/abs/2505.19659)|[langdaug](https://github.com/backpropagator/langdaug)|
|2505.19692|[drivecamsim: generalizable camera simulation via explicit camera modeling for autonomous driving](https://arxiv.org/abs/2505.19692)|[drivecamsim](https://github.com/swc-17/drivecamsim)|
|2505.19742|[haodiff: human-aware one-step diffusion via dual-prompt guidance](https://arxiv.org/abs/2505.19742)|[haodiff](https://github.com/gobunu/haodiff)|
|2505.19779|[advancements in medical image classification through fine-tuning natural domain foundation models](https://arxiv.org/abs/2505.19779)|[medical-transfer-learning](https://github.com/sajjad-sh33/medical-transfer-learning)|
|2505.19793|[depth-guided bundle sampling for efficient generalizable neural radiance field reconstruction](https://arxiv.org/abs/2505.19793)|[gdb-nerf](https://github.com/klmav-cuc/gdb-nerf)|
|2505.19799|[a regularization-guided equivariant approach for image restoration](https://arxiv.org/abs/2505.19799)|[eq-reg](https://github.com/yulu919/eq-reg)|
|2505.19805|[translation-equivariance of normalization layers and aliasing in convolutional neural networks](https://arxiv.org/abs/2505.19805)|[normalization-layers](https://github.com/jscanvic/normalization-layers)|
|2505.19812|[efficient multi-modal long context learning for training-free adaptation](https://arxiv.org/abs/2505.19812)|[emloc](https://github.com/zehong-ma/emloc)|
|2505.19813|[golf-nrt: integrating global context and local geometry for few-shot view synthesis](https://arxiv.org/abs/2505.19813)|[golf-nrt](https://github.com/klmav-cuc/golf-nrt)|
|2505.19863|[fruitnerf++: a generalized multi-fruit counting method utilizing contrastive learning and neural radiance fields](https://arxiv.org/abs/2505.19863)|[fruitnerfpp](https://github.com/meyerls/fruitnerfpp)|
|2505.19877|[vad-r1: towards video anomaly reasoning via perception-to-cognition chain-of-thought](https://arxiv.org/abs/2505.19877)|[vad-r1](https://github.com/wbfwonderful/vad-r1)|
|2505.19889|[omnifall: a unified staged-to-wild benchmark for human fall detection](https://arxiv.org/abs/2505.19889)|[omnifall-experiments](https://github.com/simplexsigil/omnifall-experiments)|
|2505.19972|[phi: bridging domain shift in long-term action quality assessment via progressive hierarchical instruction](https://arxiv.org/abs/2505.19972)|[phi_aqa](https://github.com/zhoukanglei/phi_aqa)|
|2505.20024|[reasonplan: unified scene prediction and decision reasoning for closed-loop autonomous driving](https://arxiv.org/abs/2505.20024)|[reasonplan](https://github.com/liuxueyi/reasonplan)|
|2505.20038|[towards video to piano music generation with chain-of-perform support benchmarks](https://arxiv.org/abs/2505.20038)|[video-to-audio-and-piano](https://github.com/acappemin/video-to-audio-and-piano)|
|2505.20049|[data-free class-incremental gesture recognition with prototype-guided pseudo feature replay](https://arxiv.org/abs/2505.20049)|[pgpfr-3](https://github.com/sunao-101/pgpfr-3)|
|2505.20053|[multimodal llm-guided semantic correction in text-to-image diffusion](https://arxiv.org/abs/2505.20053)|[ppad](https://github.com/hellozicky/ppad)|
|2505.20107|[refining few-step text-to-multiview diffusion via reinforcement learning](https://arxiv.org/abs/2505.20107)|[mvc-zigal](https://github.com/ziyizhang27/mvc-zigal)|
|2505.20124|[tuna: comprehensive fine-grained temporal understanding evaluation on dense dynamic videos](https://arxiv.org/abs/2505.20124)|[TUNA](https://github.com/friedrichor/TUNA)|
|2505.20126|[ob3d: a new dataset for benchmarking omnidirectional 3d reconstruction using blender](https://arxiv.org/abs/2505.20126)|[omnidirectional_blender_3d_dataset](https://github.com/gsisaoki/omnidirectional_blender_3d_dataset)|
|2505.20152|[hard negative contrastive learning for fine-grained geometric understanding in large multimodal models](https://arxiv.org/abs/2505.20152)|[mmgeolm](https://github.com/thu-keg/mmgeolm)|
|2505.20156|[hunyuanvideo-avatar: high-fidelity audio-driven human animation for multiple characters](https://arxiv.org/abs/2505.20156)|[hunyuanvideo-avatar](https://github.com/tencent-hunyuan/hunyuanvideo-avatar)|
|2505.20255|[anicrafter: customizing realistic human-centric animation via avatar-background conditioning in video diffusion models](https://arxiv.org/abs/2505.20255)|[anicrafter](https://github.com/myniuuu/anicrafter)|
|2505.20256|[omni-r1: reinforcement learning for omnimodal reasoning via two-system collaboration](https://arxiv.org/abs/2505.20256)|[omni-r1](https://github.com/aim-uofa/omni-r1)|
|2505.20275|[imgedit: a unified image editing dataset and benchmark](https://arxiv.org/abs/2505.20275)|[imgedit](https://github.com/pku-yuangroup/imgedit)|
|2505.20277|[omnicharacter: towards immersive role-playing agents with seamless speech-language personality interaction](https://arxiv.org/abs/2505.20277)|[damo-convai](https://github.com/alibabaresearch/damo-convai)|
|2505.20288|[hierarchical masked autoregressive models with low-resolution token pivots](https://arxiv.org/abs/2505.20288)|[himar](https://github.com/hidream-ai/himar)|
|2505.20291|[visualized text-to-image retrieval](https://arxiv.org/abs/2505.20291)|[visualize-then-retrieve](https://github.com/xiaowu0162/visualize-then-retrieve)|
|2505.20297|[disa: diffusion step annealing in autoregressive image generation](https://arxiv.org/abs/2505.20297)|[disa](https://github.com/qinyu-allen-zhao/disa)|
|2505.20298|[mangavqa and mangalmm: a benchmark and specialized model for multimodal manga understanding](https://arxiv.org/abs/2505.20298)|[mangalmm](https://github.com/manga109/mangalmm)|
|2505.01476|[costfilter-ad: enhancing anomaly detection through matching cost filtering](https://arxiv.org/abs/2505.01476)|[costfilter-ad](https://github.com/zhe-sapi/costfilter-ad)|
|2505.05528|[x-transfer attacks: towards super transferable adversarial attacks on clip](https://arxiv.org/abs/2505.05528)|[XTransferBench](https://github.com/HanxunH/XTransferBench)|
|2505.10541|[exploring implicit visual misunderstandings in multimodal large language models through attention analysis](https://arxiv.org/abs/2505.10541)|[stme](https://github.com/welldonepf/stme)|
|2505.10595|[arfc-wahnet: adaptive receptive field convolution and wavelet-attentive hierarchical network for infrared small target detection](https://arxiv.org/abs/2505.10595)|[arfc-wahnet](https://github.com/leaf2001/arfc-wahnet)|
|2505.11454|[humanibench: a human-centric framework for large multimodal models evaluation](https://arxiv.org/abs/2505.11454)|[humanibench](https://github.com/vectorinstitute/humanibench)|
|2505.14151|[reactdiff: latent diffusion for facial reaction generation](https://arxiv.org/abs/2505.14151)|[reactdiff](https://github.com/hunan-tiger/reactdiff)|
|2505.15425|[on the robustness of medical vision-language models: are they truly generalizable?](https://arxiv.org/abs/2505.15425)|[robustmedclip](https://github.com/biomedia-mbzuai/robustmedclip)|
|2505.16809|[hypergraph tversky-aware domain incremental learning for brain tumor segmentation with missing modalities](https://arxiv.org/abs/2505.16809)|[rehydil](https://github.com/reeive/rehydil)|
|2505.16839|[lavida: a large diffusion language model for multimodal understanding](https://arxiv.org/abs/2505.16839)|[lavida](https://github.com/jacklishufan/lavida)|
|2505.16854|[think or not? selective reasoning via reinforcement learning for vision-language models](https://arxiv.org/abs/2505.16854)|[ton](https://github.com/kokolerk/ton)|
|2505.17114|[raven: query-guided representation alignment for question answering over audio, video, embedded sensors, and natural language](https://arxiv.org/abs/2505.17114)|[raven](https://github.com/bashlab/raven)|
|2505.17241|[generative ai and creativity: a systematic literature review and meta-analysis](https://arxiv.org/abs/2505.17241)|[meta-analysis-llms-creativity](https://github.com/sm2982/meta-analysis-llms-creativity)|
|2505.17440|[veattack: downstream-agnostic vision encoder attack against large vision language models](https://arxiv.org/abs/2505.17440)|[veattack-lvlm](https://github.com/hfmei/veattack-lvlm)|
|2505.17475|[posebh: prototypical multi-dataset training beyond human pose estimation](https://arxiv.org/abs/2505.17475)|[PoseBH](https://github.com/uyoung-jeong/PoseBH)|
|2505.17551|[center-aware residual anomaly synthesis for multi-class industrial anomaly detection](https://arxiv.org/abs/2505.17551)|[CRAS](https://github.com/cqylunlun/CRAS)|
|2505.17556|[wildfire spread forecasting with deep learning](https://arxiv.org/abs/2505.17556)|[wildfirespread](https://github.com/orion-ai-lab/wildfirespread)|
|2505.17581|[modem: a morton-order degradation estimation mechanism for adverse weather image recovery](https://arxiv.org/abs/2505.17581)|[modem](https://github.com/hainuo-wang/modem)|
|2505.17591|[minkunext-si: improving point cloud-based place recognition including spherical coordinates and lidar intensity](https://arxiv.org/abs/2505.17591)|[minkunext-si](https://github.com/judithv/minkunext-si)|
|2505.17739|[feasible action space reduction for quantifying causal responsibility in continuous spatial interactions](https://arxiv.org/abs/2505.17739)|[continuousfear](https://github.com/dai-lab-herald/continuousfear)|
|2505.17771|[topopoint: enhance topology reasoning via endpoint detection in autonomous driving](https://arxiv.org/abs/2505.17771)|[topopoint](https://github.com/franpin/topopoint)|
|2505.17807|[temporal consistency constrained transferable adversarial attacks with background mixup for action recognition](https://arxiv.org/abs/2505.17807)|[bmtc_transferattackvid](https://github.com/mlvccn/bmtc_transferattackvid)|
|2505.17883|[fastcav: efficient computation of concept activation vectors for explaining deep neural networks](https://arxiv.org/abs/2505.17883)|[fastcav](https://gitlab.com/dlr-dw/fastcav)|
|2505.17884|[track anything annotate: video annotation and dataset generation of computer vision models](https://arxiv.org/abs/2505.17884)|[track-anything-annotate](https://github.com/lnikioffic/track-anything-annotate)|
|2505.17911|[object-level cross-view geo-localization with location enhancement and multi-head cross attention](https://arxiv.org/abs/2505.17911)|[ocgnet](https://github.com/zheyangh/ocgnet)|
|2505.17937|[survival games: human-llm strategic showdowns under severe resource scarcity](https://arxiv.org/abs/2505.17937)|[survival-games](https://github.com/hong123123/survival-games)|
|2505.18015|[semsegbench & detecbench: benchmarking reliability and generalization beyond classification](https://arxiv.org/abs/2505.18015)|[benchmarking_reliability_generalization](https://github.com/shashankskagnihotri/benchmarking_reliability_generalization)|
|2505.18028|[knot so simple: a minimalistic environment for spatial reasoning](https://arxiv.org/abs/2505.18028)|[knotgym](https://github.com/lil-lab/knotgym)|
|2505.18035|[camme: adaptive deepfake image detection with multi-modal cross-attention](https://arxiv.org/abs/2505.18035)|[camme](https://github.com/magnet300/camme)|
|2505.18153|[ren: fast and efficient region encodings from patch-based image encoders](https://arxiv.org/abs/2505.18153)|[ren](https://github.com/savya08/ren)|
|2505.02567|[unified multimodal understanding and generation models: advances, challenges, and opportunities](https://arxiv.org/abs/2505.02567)|[awesome-unified-multimodal-models](https://github.com/aidc-ai/awesome-unified-multimodal-models)|
|2505.10049|[advances in radiance field for dynamic scene: from neural field to gaussian field](https://arxiv.org/abs/2505.10049)|[dynamic-radiation-field-paper-list](https://github.com/moonflo/dynamic-radiation-field-paper-list)|
|2505.10250|[adhmr: aligning diffusion-based human mesh recovery via direct preference optimization](https://arxiv.org/abs/2505.10250)|[adhmr](https://github.com/shenwenhao01/adhmr)|
|2505.10473|[consistent quantity-quality control across scenes for deployment-aware gaussian splatting](https://arxiv.org/abs/2505.10473)|[ControlGS](https://github.com/zhang-fengdi/ControlGS)|
|2505.12007|[multi-modal collaborative optimization and expansion network for event-assisted single-eye expression recognition](https://arxiv.org/abs/2505.12007)|[MCO-E-Net](https://github.com/hrdhrd/MCO-E-Net)|
|2505.12081|[visionreasoner: unified visual perception and reasoning via reinforcement learning](https://arxiv.org/abs/2505.12081)|[easyr1](https://github.com/hiyouga/easyr1)|
|2505.14068|[place recognition: a comprehensive review, current challenges and future directions](https://arxiv.org/abs/2505.14068)|[sota-place-recognitioner](https://github.com/cv4ra/sota-place-recognitioner)|
|2505.15222|[continuous representation methods, theories, and applications: an overview and perspectives](https://arxiv.org/abs/2505.15222)|[continuous-representation-zoo](https://github.com/yisiluo/continuous-representation-zoo)|
|2505.15358|[objective bicycle occlusion level classification using a deformable parts-based model](https://arxiv.org/abs/2505.15358)|[Bicycle_Occlusion_Level](https://github.com/angelmangubat/Bicycle_Occlusion_Level)|
|2505.15441|[stronger vits with octic equivariance](https://arxiv.org/abs/2505.15441)|[octic-vits](https://github.com/davnords/octic-vits)|
|2505.15810|[gui-g1: understanding r1-zero-like training for visual grounding in gui agents](https://arxiv.org/abs/2505.15810)|[gui-g1](https://github.com/yuqi-zhou/gui-g1)|
|2505.15867|[scenir: visual semantic clarity through unsupervised scene graph retrieval](https://arxiv.org/abs/2505.15867)|[scenir-icml2025](https://github.com/nickhaidos/scenir-icml2025)|
|2505.15870|[satellites reveal mobility: a commuting origin-destination flow generator for global cities](https://arxiv.org/abs/2505.15870)|[generate-od-pubtools](https://github.com/tsinghua-fib-lab/generate-od-pubtools)|
|2505.15928|[viqagent: zero-shot video question answering via agent with open-vocabulary grounding validation](https://arxiv.org/abs/2505.15928)|[viqagent](https://github.com/t-montes/viqagent)|
|2505.16029|[learning better representations for crowded pedestrians in offboard lidar-camera 3d tracking-by-detection](https://arxiv.org/abs/2505.16029)|[pcp-mv](https://github.com/nicholasli1995/pcp-mv)|
|2505.16104|[hierarchical safety realignment: lightweight restoration of safety in pruned large vision-language models](https://arxiv.org/abs/2505.16104)|[hsr](https://github.com/theshineyue/hsr)|
|2505.16161|[deep learning-driven ultra-high-definition image restoration: a survey](https://arxiv.org/abs/2505.16161)|[uhd-image-restoration-survey](https://github.com/wlydlut/uhd-image-restoration-survey)|
|2505.16165|[re-trip : reflectivity instance augmented triangle descriptor for 3d place recognition](https://arxiv.org/abs/2505.16165)|[re-trip](https://github.com/pyc5714/re-trip)|
|2505.16175|[quickvideo: real-time long video understanding with system algorithm co-design](https://arxiv.org/abs/2505.16175)|[quickvideo](https://github.com/tiger-ai-lab/quickvideo)|
|2505.16264|[linea: fast and accurate line detection using scalable transformers](https://arxiv.org/abs/2505.16264)|[LINEA](https://github.com/SebastianJanampa/LINEA)|
|2505.16282|[arpo:end-to-end policy optimization for gui agents with experience replay](https://arxiv.org/abs/2505.16282)|[arpo](https://github.com/dvlab-research/arpo)|
|2505.16313|[accelerating targeted hard-label adversarial attacks in low-query black-box settings](https://arxiv.org/abs/2505.16313)|[tea](https://github.com/mdppml/tea)|
|2505.16321|[efficient motion prompt learning for robust visual tracking](https://arxiv.org/abs/2505.16321)|[motion-prompt-tracking](https://github.com/zj5559/motion-prompt-tracking)|
|2505.16335|[fpqvar: floating point quantization for visual autoregressive model with fpga hardware co-design](https://arxiv.org/abs/2505.16335)|[fpqvar](https://github.com/pku-sec-lab/fpqvar)|
|2505.16360|[style transfer with diffusion models for synthetic-to-real domain adaptation](https://arxiv.org/abs/2505.16360)|[cactif](https://github.com/echigot/cactif)|
|2505.16376|[decafnet: delegate and conquer for efficient temporal grounding in long videos](https://arxiv.org/abs/2505.16376)|[cvpr2025-decafnet](https://github.com/zijialewislu/cvpr2025-decafnet)|
|2505.16402|[advreal: adversarial patch generation framework with application to adversarial safety evaluation of object detection systems](https://arxiv.org/abs/2505.16402)|[advreal](https://github.com/huangyh98/advreal)|
|2505.16411|[mitigating hallucinations in vision-language models through image-guided head suppression](https://arxiv.org/abs/2505.16411)|[spin](https://github.com/yueche77/spin)|
|2505.16416|[circle-rope: cone-like decoupled rotary positional embedding for large vision-language models](https://arxiv.org/abs/2505.16416)|[circlerope](https://github.com/lose4578/circlerope)|
|2505.16441|[ranked entropy minimization for continual test-time adaptation](https://arxiv.org/abs/2505.16441)|[rem](https://github.com/pilshan/rem)|
|2505.16470|[benchmarking retrieval-augmented multimomal generation for document question answering](https://arxiv.org/abs/2505.16470)|[mmdocrag](https://github.com/mmdocrag/mmdocrag)|
|2505.16495|[alto: adaptive-length tokenizer for autoregressive mask generation](https://arxiv.org/abs/2505.16495)|[altollm](https://github.com/yayafengzi/altollm)|
|2505.16579|[bridging the dynamic perception gap: training-free draft chain-of-thought for dynamic multimodal spatial reasoning](https://arxiv.org/abs/2505.16579)|[d2r](https://github.com/cratileo/d2r)|
|2505.16625|[background matters: a cross-view bidirectional modeling framework for semi-supervised medical image segmentation](https://arxiv.org/abs/2505.16625)|[cvbm](https://github.com/caoluyang0830/cvbm)|
|2505.16650|[unsupervised network anomaly detection with autoencoders and traffic images](https://arxiv.org/abs/2505.16650)|[image-based-network-traffic-anomaly-detection](https://github.com/michaelneri/image-based-network-traffic-anomaly-detection)|
|2505.16658|[zero-shot hyperspectral pansharpening using hysteresis-based tuning for spectral quality control](https://arxiv.org/abs/2505.16658)|[rho-pnn](https://github.com/giu-guarino/rho-pnn)|
|2505.16663|[conav: collaborative cross-modal reasoning for embodied navigation](https://arxiv.org/abs/2505.16663)|[CoNav](https://github.com/oceanhao/CoNav)|
|2505.16673|[r1-sharevl: incentivizing reasoning capability of multimodal large language models via share-grpo](https://arxiv.org/abs/2505.16673)|[r1-sharevl](https://github.com/hjyao00/r1-sharevl)|
|2505.16685|[on the use of graphs for satellite image time series](https://arxiv.org/abs/2505.16685)|[graph4sits](https://github.com/corentin-dfg/graph4sits)|
|2505.16740|[robust vision-based runway detection through conformal prediction and conformal map](https://arxiv.org/abs/2505.16740)|[conformal_runway_detection](https://github.com/alyasltd/conformal_runway_detection)|
|2505.16778|[single domain generalization for few-shot counting via universal representation matching](https://arxiv.org/abs/2505.16778)|[urm](https://github.com/jbr97/urm)|
|2505.16792|[repa works until it doesn't: early-stopped, holistic alignment supercharges diffusion training](https://arxiv.org/abs/2505.16792)|[haste](https://github.com/nus-hpc-ai-lab/haste)|
|2505.16793|[reobench: benchmarking robustness of earth observation foundation models](https://arxiv.org/abs/2505.16793)|[reobench](https://github.com/lx709/reobench)|
|2505.16815|[perceptual quality assessment for embodied ai](https://arxiv.org/abs/2505.16815)|[embodiediqa](https://github.com/lcysyzxdxc/embodiediqa)|
|2505.16832|[from eduvisbench to eduvisagent: a benchmark and multi-agent framework for pedagogical visualization](https://arxiv.org/abs/2505.16832)|[eduvisbench](https://github.com/aiming-lab/eduvisbench)|
|2505.16850|[atr-bench: a federated learning benchmark for adaptation, trust, and reasoning](https://arxiv.org/abs/2505.16850)|[atr-bench](https://github.com/tajamul21/atr-bench)|
|2505.16864|[training-free efficient video generation via dynamic token carving](https://arxiv.org/abs/2505.16864)|[jenga](https://github.com/dvlab-research/jenga)|
|2505.16902|[realengine: simulating autonomous driving in realistic context](https://arxiv.org/abs/2505.16902)|[realengine](https://github.com/fudan-zvg/realengine)|
|2505.16916|[backdoor cleaning without external guidance in mllm fine-tuning](https://arxiv.org/abs/2505.16916)|[bye](https://github.com/xuankunrong/bye)|
|2505.16974|[openseg-r: improving open-vocabulary segmentation via step-by-step visual reasoning](https://arxiv.org/abs/2505.16974)|[openseg-r](https://github.com/hanzy1996/openseg-r)|
|2505.16977|[incorporating visual correspondence into diffusion model for virtual try-on](https://arxiv.org/abs/2505.16977)|[spm-diff](https://github.com/hidream-ai/spm-diff)|
|2505.16985|[extremely simple multimodal outlier synthesis for out-of-distribution detection and segmentation](https://arxiv.org/abs/2505.16985)|[featuremixing](https://github.com/mona4399/featuremixing)|
|2505.17002|[paeff: precise alignment and enhanced gated feature fusion for face-voice association](https://arxiv.org/abs/2505.17002)|[paeff](https://github.com/hannabdul/paeff)|
|2505.17008|[deep mineralogical segmentation of thin section images based on qemscan maps](https://arxiv.org/abs/2505.17008)|[deep-mineralogical-segmentation](https://github.com/ltracegeo/deep-mineralogical-segmentation)|
|2505.17011|[learning adaptive and temporally causal video tokenization in a 1d latent space](https://arxiv.org/abs/2505.17011)|[adaptok](https://github.com/visionxlab/adaptok)|
|2505.17012|[spatialscore: towards unified evaluation for multimodal spatial understanding](https://arxiv.org/abs/2505.17012)|[SpatialScore](https://github.com/haoningwu3639/SpatialScore)|
|2505.17017|[delving into rl for image generation with cot: a study on dpo vs. grpo](https://arxiv.org/abs/2505.17017)|[image-generation-cot](https://github.com/ziyuguo99/image-generation-cot)|
|2505.17018|[sophiavl-r1: reinforcing mllms reasoning with thinking reward](https://arxiv.org/abs/2505.17018)|[sophiavl-r1](https://github.com/kxfan2002/sophiavl-r1)|
|2505.17019|[let androids dream of electric sheep: a human-like image implication understanding and reasoning framework](https://arxiv.org/abs/2505.17019)|[let-androids-dream-of-electric-sheep](https://github.com/ming-zch/let-androids-dream-of-electric-sheep)|
|2505.17020|[crosslmm: decoupling long video sequences from lmms via dual cross-attention mechanisms](https://arxiv.org/abs/2505.17020)|[crosslmm](https://github.com/shilinyan99/crosslmm)|
|2505.17021|[arb: a comprehensive arabic multimodal reasoning benchmark](https://arxiv.org/abs/2505.17021)|[arb](https://github.com/mbzuai-oryx/arb)|
|2505.17022|[got-r1: unleashing reasoning capability of mllm for visual generation with reinforcement learning](https://arxiv.org/abs/2505.17022)|[got-r1](https://github.com/gogoduan/got-r1)|
|2505.01237|[cav-mae sync: improving contrastive audio-visual mask autoencoders via fine-grained alignment](https://arxiv.org/abs/2505.01237)|[cav-mae-sync](https://github.com/edsonroteia/cav-mae-sync)|
|2505.04046|[reliable disentanglement multi-view learning against view adversarial attacks](https://arxiv.org/abs/2505.04046)|[2025-IJCAI-RDML](https://github.com/Willy1005/2025-IJCAI-RDML)|
|2505.04788|[convex relaxation for robust vanishing point estimation in manhattan world](https://arxiv.org/abs/2505.04788)|[globustvp](https://github.com/wu-cvgl/globustvp)|
|2505.05049|[uncertainsam: fast and efficient uncertainty quantification of the segment anything model](https://arxiv.org/abs/2505.05049)|[UncertainSAM](https://github.com/GreenAutoML4FAS/UncertainSAM)|
|2505.05071|[fg-clip: fine-grained visual and textual alignment](https://arxiv.org/abs/2505.05071)|[fg-clip](https://github.com/360cvgroup/fg-clip)|
|2505.12620|[busterx: mllm-powered ai-generated video forgery detection and explanation](https://arxiv.org/abs/2505.12620)|[busterx](https://github.com/l8cv/busterx)|
|2505.13300|[dd-ranking: rethinking the evaluation of dataset distillation](https://arxiv.org/abs/2505.13300)|[dd-ranking](https://github.com/nus-hpc-ai-lab/dd-ranking)|
|2505.14074|[recreating neural activity during speech production with language and speech model embeddings](https://arxiv.org/abs/2505.14074)|[llm_brain_representations](https://github.com/owaismujtaba/llm_brain_representations)|
|2505.14100|[unlocking the power of sam 2 for few-shot segmentation](https://arxiv.org/abs/2505.14100)|[fssam](https://github.com/sam1224/fssam)|
|2505.14707|[crypticbio: a large multimodal dataset for visually confusing biodiversity](https://arxiv.org/abs/2505.14707)|[crypticbio](https://github.com/georgianagmanolache/crypticbio)|
|2505.14708|[draftattention: fast video diffusion via low-resolution attention guidance](https://arxiv.org/abs/2505.14708)|[draft-attention](https://github.com/shawnricecake/draft-attention)|
|2505.14709|[fastcar: cache attentive replay for fast auto-regressive video generation on the edge](https://arxiv.org/abs/2505.14709)|[fast-car](https://github.com/shawnricecake/fast-car)|
|2505.14714|[kgalign: joint semantic-structural knowledge encoding for multimodal fake news detection](https://arxiv.org/abs/2505.14714)|[kgalign](https://github.com/latuanvinh1998/kgalign)|
|2505.14717|[aneumo: a large-scale multimodal aneurysm dataset with computational fluid dynamics simulations and deep learning benchmarks](https://arxiv.org/abs/2505.14717)|[aneumo](https://github.com/xigui-li/aneumo)|
|2505.14747|[lod1 3d city model from lidar: the impact of segmentation accuracy on quality of urban 3d modeling and morphology extraction](https://arxiv.org/abs/2505.14747)|[LiDAR-3D-Building-Modeling](https://github.com/FatemehCh97/LiDAR-3D-Building-Modeling)|
|2505.14846|[open-set semi-supervised learning for long-tailed medical datasets](https://arxiv.org/abs/2505.14846)|[openltr](https://github.com/daniyanaj/openltr)|
|2505.14931|[colors matter: ai-driven exploration of human feature colors](https://arxiv.org/abs/2505.14931)|[Color-Matters-AI-Driven-Exploration-of-Human-Feature-Coloration](https://github.com/AiTaif7/Color-Matters-AI-Driven-Exploration-of-Human-Feature-Coloration)|
|2505.14948|[programmatic video prediction using large language models](https://arxiv.org/abs/2505.14948)|[ProgGen](https://github.com/metro-smiles/ProgGen)|
|2505.14951|[multimae meets earth observation: pre-training multi-modal multi-task masked autoencoders for earth observation tasks](https://arxiv.org/abs/2505.14951)|[multimae-meets-eo](https://github.com/josesosajs/multimae-meets-eo)|
|2505.14983|[toward informed av decision-making: computational model of well-being and trust in mobility](https://arxiv.org/abs/2505.14983)|[wellbeing-trust-model](https://github.com/honda-research-institute/wellbeing-trust-model)|
|2505.15031|[are the confidence scores of reviewers consistent with the review content? evidence from top conference proceedings in ai](https://arxiv.org/abs/2505.15031)|[confidence_score](https://github.com/njust-winchy/confidence_score)|
|2505.15075|[traveling across languages: benchmarking cross-lingual consistency in multimodal llms](https://arxiv.org/abs/2505.15075)|[traveling-across-languages](https://github.com/nlp-waseda/traveling-across-languages)|
|2505.15111|[ipad: iterative proposal-centric end-to-end autonomous driving](https://arxiv.org/abs/2505.15111)|[iPad](https://github.com/Kguo-cs/iPad)|
|2505.15120|[lung nodule-ssm: self-supervised lung nodule detection and classification in thoracic ct images](https://arxiv.org/abs/2505.15120)|[lung-nodule-ssm-self-supervised-lung-nodule-detection-and-classification](https://github.com/emeraldsnrpu/lung-nodule-ssm-self-supervised-lung-nodule-detection-and-classification)|
|2505.15133|[deepkd: a deeply decoupled and denoised knowledge distillation trainer](https://arxiv.org/abs/2505.15133)|[deepkd](https://github.com/haiduo/deepkd)|
|2505.15145|[cinetechbench: a benchmark for cinematographic technique understanding and generation](https://arxiv.org/abs/2505.15145)|[cinetechbench](https://github.com/pris-cv/cinetechbench)|
|2505.15184|[auxdet: auxiliary metadata matters for omni-domain infrared small target detection](https://arxiv.org/abs/2505.15184)|[auxdet](https://github.com/grokcv/auxdet)|
|2505.15185|[monosplat: generalizable 3d gaussian splatting from monocular depth foundation models](https://arxiv.org/abs/2505.15185)|[monosplat](https://github.com/cuhk-aim-group/monosplat)|
|2505.15217|[multimodal conditional information bottleneck for generalizable ai-generated image detection](https://arxiv.org/abs/2505.15217)|[infofd](https://github.com/ant0ny44/infofd)|
|2505.15232|[dc-scene: data-centric learning for 3d scene understanding](https://arxiv.org/abs/2505.15232)|[dc-scene](https://github.com/aigeeksgroup/dc-scene)|
|2505.15234|[sama-unet: enhancing medical image segmentation with self-adaptive mamba-like attention and causal-resonance learning](https://arxiv.org/abs/2505.15234)|[SAMA-UNet](https://github.com/sqbqamar/SAMA-UNet)|
|2505.15270|[scaling diffusion transformers efficiently via $\mu$p](https://arxiv.org/abs/2505.15270)|[Scaling-Diffusion-Transformers-muP](https://github.com/ML-GSAI/Scaling-Diffusion-Transformers-muP)|
|2505.15272|[diffprob: data pruning for face recognition](https://arxiv.org/abs/2505.15272)|[DiffProb](https://github.com/EduardaCaldeira/DiffProb)|
|2505.15282|[exploring in-image machine translation with real-world background](https://arxiv.org/abs/2505.15282)|[debackx](https://github.com/bithlp/debackx)|
|2505.15284|[kernel pca for out-of-distribution detection: non-linear kernel selections and approximations](https://arxiv.org/abs/2505.15284)|[ood-kpca-extension](https://github.com/fanghenshaometeor/ood-kpca-extension)|
|2505.15325|[softhgnn: soft hypergraph neural networks for general visual recognition](https://arxiv.org/abs/2505.15325)|[SoftHGNN](https://github.com/Mengqi-Lei/SoftHGNN)|
|2505.15364|[mhanet: multi-scale hybrid attention network for auditory attention detection](https://arxiv.org/abs/2505.15364)|[mhanet](https://github.com/fchest/mhanet)|
|2505.15379|[the p$^3$ dataset: pixels, points and polygons for multimodal building vectorization](https://arxiv.org/abs/2505.15379)|[pixelspointspolygons](https://github.com/raphaelsulzer/pixelspointspolygons)|
|2505.15435|[timecausality: evaluating the causal ability in time dimension for vision language models](https://arxiv.org/abs/2505.15435)|[timecausality](https://github.com/zeqing-wang/timecausality)|
|2505.15506|[prompt tuning vision language models with margin regularizer for few-shot learning under distribution shifts](https://arxiv.org/abs/2505.15506)|[promptmargin](https://github.com/debarshigit/promptmargin)|
|2505.15545|[seg_3d_by_pc2d: multi-view projection for domain generalization and adaptation in 3d semantic segmentation](https://arxiv.org/abs/2505.15545)|[ia4markings](https://github.com/andrewcaunes/ia4markings)|
|2505.15576|[visual perturbation and adaptive hard negative contrastive learning for compositional reasoning in vision-language models](https://arxiv.org/abs/2505.15576)|[ahnpl](https://github.com/nynu-bdai/ahnpl)|
|2505.15581|[uwsam: segment anything model guided underwater instance segmentation and a large-scale benchmark dataset](https://arxiv.org/abs/2505.15581)|[uiis10k](https://github.com/liamlian0727/uiis10k)|
|2505.15596|[exploring llm-generated feedback for economics essays: how teaching assistants evaluate and envision its use](https://arxiv.org/abs/2505.15596)|[aied2025-exploring-llm-generated-feedback-for-economics-essay](https://github.com/um-lifelong-learning-lab/aied2025-exploring-llm-generated-feedback-for-economics-essay)|
|2505.15628|[snap: a benchmark for testing the effects of capture conditions on fundamental vision tasks](https://arxiv.org/abs/2505.15628)|[snap](https://github.com/ykotseruba/snap)|
|2505.15637|[oral imaging for malocclusion issues assessments: omni dataset, deep learning baselines and benchmarking](https://arxiv.org/abs/2505.15637)|[omni](https://github.com/roundfacej/omni)|
|2505.15644|[fragfake: a dataset for fine-grained detection of edited images with vision language models](https://arxiv.org/abs/2505.15644)|[FragFake](https://github.com/Vincent-HKUSTGZ/FragFake)|
|2505.15649|[the devil is in fine-tuning and long-tailed problems:a new benchmark for scene text detection](https://arxiv.org/abs/2505.15649)|[ltb](https://github.com/pd162/ltb)|
|2505.15809|[mmada: multimodal large diffusion language models](https://arxiv.org/abs/2505.15809)|[mmada](https://github.com/gen-verse/mmada)|
|2505.15816|[streamline without sacrifice -- squeeze out computation redundancy in lmm](https://arxiv.org/abs/2505.15816)|[proxyv](https://github.com/penghao-wu/proxyv)|
|2505.15818|[instructsam: a training-free framework for instruction-oriented remote sensing object recognition](https://arxiv.org/abs/2505.15818)|[InstructSAM](https://github.com/VoyagerXvoyagerx/InstructSAM)|


## Archives
- [May 2025](archives/2025/05.md)
- [April 2025](archives/2025/04.md)
