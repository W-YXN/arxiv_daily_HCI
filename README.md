# Daily ArXiv HCI

A curated collection of arXiv papers with open-source implementations, specifically focusing on Human-Computer Interaction (cs.HC) and related fields like Computer Graphics (cs.GR), Computer Vision (cs.CV), etc. This repository aims to serve researchers and practitioners in HCI by providing easy access to papers that come with their source code implementations.

## Overview
This project automatically tracks and analyzes papers from relevant HCI categories on arXiv daily using GitHub Actions. It specifically identifies and catalogs papers that have released their source code, making it easier for researchers in HCI and related areas to find implementable research work.

The main features include:
- Daily updates of papers with open-source implementations
- Focus on Human-Computer Interaction and related research
- Automatic tracking and organization

## Latest Updates 
|date|paper|code|
|---|---|---|
|2505.04229|[a weak supervision learning approach towards an equitable mobility estimation](https://arxiv.org/abs/2505.04229)|[equitable_mobility_estimation](https://github.com/societal-computing/equitable_mobility_estimation)|
|2505.12363|[towards visuospatial cognition via hierarchical fusion of visual experts](https://arxiv.org/abs/2505.12363)|[vica](https://github.com/nkkbr/vica)|
|2505.14664|[akrmap: adaptive kernel regression for trustworthy visualization of cross-modal embeddings](https://arxiv.org/abs/2505.14664)|[akrmap](https://github.com/yilinye/akrmap)|
|2505.16832|[from eduvisbench to eduvisagent: a benchmark and multi-agent framework for reasoning-driven pedagogical visualization](https://arxiv.org/abs/2505.16832)|[eduvisbench](https://github.com/aiming-lab/eduvisbench)|
|2505.17002|[paeff: precise alignment and enhanced gated feature fusion for face-voice association](https://arxiv.org/abs/2505.17002)|[paeff](https://github.com/hannabdul/paeff)|
|2505.17937|[survival games: human-llm strategic showdowns under severe resource scarcity](https://arxiv.org/abs/2505.17937)|[survival-games](https://github.com/hong123123/survival-games)|
|2505.20292|[opens2v-nexus: a detailed benchmark and million-scale dataset for subject-to-video generation](https://arxiv.org/abs/2505.20292)|[ConsisID](https://github.com/PKU-YuanGroup/ConsisID)|
|2505.21136|[sageattention2++: a more efficient implementation of sageattention2](https://arxiv.org/abs/2505.21136)|[SageAttention](https://github.com/thu-ml/SageAttention)|
|2505.21544|[vision meets language: a rag-augmented yolov8 framework for coffee disease diagnosis and farmer assistance](https://arxiv.org/abs/2505.21544)|[A-RAG-Augmented-YOLOv8-Framework](https://github.com/semanto-mondal/A-RAG-Augmented-YOLOv8-Framework)|
|2505.21925|[renderformer: transformer-based neural rendering of triangle meshes with global illumination](https://arxiv.org/abs/2505.21925)|[renderformer](https://github.com/microsoft/renderformer)|
|2505.21954|[unitalk: towards universal active speaker detection in real world scenarios](https://arxiv.org/abs/2505.21954)|[UniTalk-ASD-code](https://github.com/plnguyen2908/UniTalk-ASD-code)|
|2505.22228|[gomatching++: parameter- and data-efficient arbitrary-shaped video text spotting and benchmarking](https://arxiv.org/abs/2505.22228)|[gomatching](https://github.com/hxyz-123/gomatching)|
|2505.22664|[zero-shot vision encoder grafting via llm surrogates](https://arxiv.org/abs/2505.22664)|[zero](https://github.com/facebookresearch/zero)|
|2505.10464|[hwa-unetr: hierarchical window aggregate unetr for 3d multimodal gastric lesion segmentation](https://arxiv.org/abs/2505.10464)|[hwa-unetr](https://github.com/jeming-creater/hwa-unetr)|
|2505.10610|[mmlongbench: benchmarking long-context vision-language models effectively and thoroughly](https://arxiv.org/abs/2505.10610)|[mmlongbench](https://github.com/edinburghnlp/mmlongbench)|
|2505.12155|[softpq: robust instance segmentation evaluation via soft matching and tunable thresholds](https://arxiv.org/abs/2505.12155)|[SoftPQ](https://github.com/rkarmaka/SoftPQ)|
|2505.12499|[rebalancing contrastive alignment with learnable semantic gaps in text-video retrieval](https://arxiv.org/abs/2505.12499)|[gare-text-video-retrieval](https://github.com/musicman217/gare-text-video-retrieval)|
|2505.13539|[eulearn: a 3d database for learning euler characteristics](https://arxiv.org/abs/2505.13539)|[eulearn_db](https://github.com/appliedgeometry/eulearn_db)|
|2505.15812|[leveraging the powerful attention of a pre-trained diffusion model for exemplar-based image colorization](https://arxiv.org/abs/2505.15812)|[powerful-attention](https://github.com/satoshi-kosugi/powerful-attention)|
|2505.17013|[when are concepts erased from diffusion models?](https://arxiv.org/abs/2505.17013)|[diffusionconcepterasure](https://github.com/kevinlu4588/diffusionconcepterasure)|
|2505.17629|[transbench: breaking barriers for transferable graphical user interface agents in dynamic digital environments](https://arxiv.org/abs/2505.17629)|[transbench](https://github.com/buaa-irip-llm/transbench)|
|2505.18022|[remotesam: towards segment anything for earth observation](https://arxiv.org/abs/2505.18022)|[RemoteSAM](https://github.com/1e12Leon/RemoteSAM)|
|2505.18060|[semantic correspondence: unified benchmarking and a strong baseline](https://arxiv.org/abs/2505.18060)|[Semantic-Correspondence](https://github.com/Visual-AI/Semantic-Correspondence)|
|2505.18958|[cdpdnet: integrating text guidance with hybrid vision encoders for medical image segmentation](https://arxiv.org/abs/2505.18958)|[cdpdnet](https://github.com/wujiong-hub/cdpdnet)|
|2505.20414|[retromotion: retrocausal motion forecasting models are instructable](https://arxiv.org/abs/2505.20414)|[future-motion](https://github.com/kit-mrt/future-motion)|
|2505.20426|[mmperspective: do mllms understand perspective? a comprehensive benchmark for perspective perception, reasoning, and robustness](https://arxiv.org/abs/2505.20426)|[MMPerspective](https://github.com/yunlong10/MMPerspective)|
|2505.20753|[understand, think, and answer: advancing visual reasoning with large multimodal models](https://arxiv.org/abs/2505.20753)|[griffon](https://github.com/jefferyzhan/griffon)|
|2505.21032|[featinv: spatially resolved mapping from feature space to input space using conditional diffusion models](https://arxiv.org/abs/2505.21032)|[FeatInv](https://github.com/AI4HealthUOL/FeatInv)|
|2505.21152|[robis: robust binary segmentation for high-resolution industrial images](https://arxiv.org/abs/2505.21152)|[robis](https://github.com/xrli-u/robis)|
|2505.21375|[geollava-8k: scaling remote-sensing multimodal large language models to 8k resolution](https://arxiv.org/abs/2505.21375)|[GeoLLaVA-8K](https://github.com/MiliLab/GeoLLaVA-8K)|
|2505.04119|[gaprompt: geometry-aware point cloud prompt for 3d vision model](https://arxiv.org/abs/2505.04119)|[icml2025-vgp](https://github.com/zhoujiahuan1991/icml2025-vgp)|
|2505.04121|[vision graph prompting via semantic low-rank decomposition](https://arxiv.org/abs/2505.04121)|[icml2025-vgp](https://github.com/zhoujiahuan1991/icml2025-vgp)|
|2505.08614|[waveguard: robust deepfake detection and source tracing via dual-tree complex wavelet and graph neural networks](https://arxiv.org/abs/2505.08614)|[waveguard](https://github.com/vpsg-research/waveguard)|
|2505.11131|[one image is worth a thousand words: a usability preservable text-image collaborative erasing framework](https://arxiv.org/abs/2505.11131)|[co-erasing](https://github.com/ferry-li/co-erasing)|
|2505.12266|[pmq-ve: progressive multi-frame quantization for video enhancement](https://arxiv.org/abs/2505.12266)|[pmq-ve](https://github.com/xiaobigfeng/pmq-ve)|
|2505.12513|[globalgeotree: a multi-granular vision-language dataset for global tree species classification](https://arxiv.org/abs/2505.12513)|[globalgeotree](https://github.com/muyang99/globalgeotree)|
|2505.12728|[flash: latent-aware semi-autoregressive speculative decoding for multimodal tasks](https://arxiv.org/abs/2505.12728)|[flashsd](https://github.com/zihuaevan/flashsd)|
|2505.13061|[3d visual illusion depth estimation](https://arxiv.org/abs/2505.13061)|[3d-visual-illusion-depth-estimation](https://github.com/yaochengtang/3d-visual-illusion-depth-estimation)|
|2505.13740|[improving compositional generation with diffusion models using lift scores](https://arxiv.org/abs/2505.13740)|[complift](https://github.com/rainorangelemon/complift)|
|2505.14362|[deepeyes: incentivizing "thinking with images" via reinforcement learning](https://arxiv.org/abs/2505.14362)|[deepeyes](https://github.com/visual-agent/deepeyes)|
|2505.15235|[x-grm: large gaussian reconstruction model for sparse-view x-rays to computed tomography](https://arxiv.org/abs/2505.15235)|[x-grm](https://github.com/cuhk-aim-group/x-grm)|
|2505.15660|[exploring the limits of vision-language-action manipulations in cross-task generalization](https://arxiv.org/abs/2505.15660)|[X-ICM](https://github.com/jiaming-zhou/X-ICM)|
|2505.16882|[tracking the flight: exploring a computational framework for analyzing escape responses in plains zebra (equus quagga)](https://arxiv.org/abs/2505.16882)|[zebras-stitching](https://github.com/neuroinformatics-unit/zebras-stitching)|
|2505.16990|[dimple: discrete diffusion multimodal large language model with parallel decoding](https://arxiv.org/abs/2505.16990)|[dimple](https://github.com/yu-rp/dimple)|
|2505.18156|[injectlab: a tactical framework for adversarial threat modeling against large language models](https://arxiv.org/abs/2505.18156)|[injectlab](https://github.com/ahow2004/injectlab)|
|2505.18175|[evaluation in eeg emotion recognition: state-of-the-art review and unified framework](https://arxiv.org/abs/2505.18175)|[eegain](https://github.com/emotionlab/eegain)|
|2505.18197|[a novel benchmark and dataset for efficient 3d gaussian splatting with gaussian point cloud compression](https://arxiv.org/abs/2505.18197)|[GausPcc](https://github.com/Wangkkklll/GausPcc)|
|2505.18412|[rehabilitation exercise quality assessment and feedback generation using large language models with prompt engineering](https://arxiv.org/abs/2505.18412)|[exercisellm](https://github.com/jessicaxtang/exercisellm)|
|2505.18487|[grounding bodily awareness in visual representations for efficient policy learning](https://arxiv.org/abs/2505.18487)|[icon](https://github.com/henrywjl/icon)|
|2505.18536|[reinforcement fine-tuning powers reasoning capability of multimodal large language models](https://arxiv.org/abs/2505.18536)|[awesome-rl-based-reasoning-mllms](https://github.com/sun-haoyuan23/awesome-rl-based-reasoning-mllms)|
|2505.18546|[reflectgan: modeling vegetation effects for soil carbon estimation from satellite imagery](https://arxiv.org/abs/2505.18546)|[reflectgan](https://github.com/dristidatta/reflectgan)|
|2505.18547|[diffusion blend: inference-time multi-preference alignment for diffusion models](https://arxiv.org/abs/2505.18547)|[db-2025](https://github.com/bluewoods127/db-2025)|
|2505.18582|[on denoising walking videos for gait recognition](https://arxiv.org/abs/2505.18582)|[opengait](https://github.com/shiqiyu/opengait)|
|2505.18614|[mavl: a multilingual audio-video lyrics dataset for animated song translation](https://arxiv.org/abs/2505.18614)|[MAVL](https://github.com/k1064190/MAVL)|
|2505.18730|[align beyond prompts: evaluating world knowledge alignment in text-to-image generation](https://arxiv.org/abs/2505.18730)|[abp](https://github.com/smile365317/abp)|
|2505.18787|[think twice before adaptation: improving adaptability of deepfake detection via online test-time adaptation](https://arxiv.org/abs/2505.18787)|[t2a-think-twice-before-adaptation](https://github.com/honghanh2104/t2a-think-twice-before-adaptation)|
|2505.18829|[litecua: computer as mcp server for computer-use agent on aios](https://arxiv.org/abs/2505.18829)|[aios](https://github.com/agiresearch/aios)|
|2505.18956|[how do images align and complement lidar? towards a harmonized multi-modal 3d panoptic segmentation](https://arxiv.org/abs/2505.18956)|[ial](https://github.com/impl-lab/ial)|
|2505.18983|[amorlip: efficient language-image pretraining via amortization](https://arxiv.org/abs/2505.18983)|[amorlip](https://github.com/haotiansun14/amorlip)|
|2505.18985|[strict: stress test of rendering images containing text](https://arxiv.org/abs/2505.18985)|[strict-bench](https://github.com/tianyu-z/strict-bench)|
|2505.18989|[spars: self-play adversarial reinforcement learning for segmentation of liver tumours](https://arxiv.org/abs/2505.18989)|[spars](https://github.com/catalinatan/spars)|
|2505.19000|[veripo: cultivating long reasoning in video-llms via verifier-gudied iterative policy optimization](https://arxiv.org/abs/2505.19000)|[veripo](https://github.com/hitsz-tmg/veripo)|
|2505.19015|[can multimodal large language models understand spatial relations?](https://arxiv.org/abs/2505.19015)|[spatialmqa](https://github.com/ziyan-xiaoyu/spatialmqa)|
|2505.19028|[infochartqa: a benchmark for multimodal question answering on infographic charts](https://arxiv.org/abs/2505.19028)|[infochartqa](https://github.com/cooldawnant/infochartqa)|
|2505.19031|[medical large vision language models with multi-image visual ability](https://arxiv.org/abs/2505.19031)|[med-mim](https://github.com/xikai97/med-mim)|
|2505.19065|[mmp-2k: a benchmark multi-labeled macro photography image quality assessment database](https://arxiv.org/abs/2505.19065)|[mmp-2k](https://github.com/future-iqa/mmp-2k)|
|2505.19084|[jodi: unification of visual generation and understanding via joint modeling](https://arxiv.org/abs/2505.19084)|[jodi](https://github.com/vipl-genun/jodi)|
|2505.19094|[satori-r1: incentivizing multimodal reasoning with spatial grounding and verifiable rewards](https://arxiv.org/abs/2505.19094)|[satori-r1](https://github.com/justairr/satori-r1)|
|2505.19120|[freqformer: image-demoir\'eing transformer via efficient frequency decomposition](https://arxiv.org/abs/2505.19120)|[freqformer](https://github.com/xyliu339/freqformer)|
|2505.19147|[shifting ai efficiency from model-centric to data-centric compression](https://arxiv.org/abs/2505.19147)|[awesome-token-level-model-compression](https://github.com/xuyang-liu16/awesome-token-level-model-compression)|
|2505.19148|[dista-net: dynamic closely-spaced infrared small target unmixing](https://arxiv.org/abs/2505.19148)|[grokcso](https://github.com/grokcv/grokcso)|
|2505.19159|[a joint learning framework with feature reconstruction and prediction for incomplete satellite image time series in agricultural semantic segmentation](https://arxiv.org/abs/2505.19159)|[joint_frp](https://github.com/wangyuze-csu/joint_frp)|
|2505.19161|[benchmarking laparoscopic surgical image restoration and beyond](https://arxiv.org/abs/2505.19161)|[surgical-image-restoration](https://github.com/pjlallen/surgical-image-restoration)|
|2505.19190|[i2moe: interpretable multimodal interaction-aware mixture-of-experts](https://arxiv.org/abs/2505.19190)|[i2moe](https://github.com/raina-xin/i2moe)|
|2505.19196|[step-level reward for free in rl-based t2i diffusion model fine-tuning](https://arxiv.org/abs/2505.19196)|[coca](https://github.com/lil-shake/coca)|
|2505.19208|[domain and task-focused example selection for data-efficient contrastive medical image segmentation](https://arxiv.org/abs/2505.19208)|[polycl](https://github.com/tbwa233/polycl)|
|2505.19218|[advancing video self-supervised learning via image foundation models](https://arxiv.org/abs/2505.19218)|[advise-video-ssl](https://github.com/jingwwu/advise-video-ssl)|
|2505.19225|[meditok: a unified tokenizer for medical image synthesis and interpretation](https://arxiv.org/abs/2505.19225)|[meditok](https://github.com/masaaki-75/meditok)|
|2505.19235|[corematching: a co-adaptive sparse inference framework with token and neuron pruning for comprehensive acceleration of vision-language models](https://arxiv.org/abs/2505.19235)|[2025-icml-corematching](https://github.com/wangqinsi1/2025-icml-corematching)|
|2505.19249|[rgc-bent: a novel dataset for bent radio galaxy classification](https://arxiv.org/abs/2505.19249)|[rgc-bent](https://github.com/mirsazzathossain/rgc-bent)|
|2505.19264|[improving novel view synthesis of 360$^\circ$ scenes in extremely sparse views by jointly training hemisphere sampled synthetic images](https://arxiv.org/abs/2505.19264)|[hemisparsegs](https://github.com/angchen-dev/hemisparsegs)|
|2505.19319|[holistic white-light polyp classification via alignment-free dense distillation of auxiliary optical chromoendoscopy](https://arxiv.org/abs/2505.19319)|[add](https://github.com/huster-hq/add)|
|2505.19434|[cstrack: enhancing rgb-x tracking via compact spatiotemporal features](https://arxiv.org/abs/2505.19434)|[cstrack](https://github.com/xiaokunfeng/cstrack)|
|2505.19455|[mm-prompt: cross-modal prompt tuning for continual visual question answering](https://arxiv.org/abs/2505.19455)|[cvqa](https://github.com/xli04/cvqa)|
|2505.19503|[locality-aware zero-shot human-object interaction detection](https://arxiv.org/abs/2505.19503)|[lain](https://github.com/oreochocolate/lain)|
|2505.19536|[flowcut: rethinking redundancy via information flow for efficient vision-language models](https://arxiv.org/abs/2505.19536)|[flowcut](https://github.com/tungchintao/flowcut)|
|2505.19546|[smart-pc: skeletal model adaptation for robust test-time training in point clouds](https://arxiv.org/abs/2505.19546)|[smart-pc](https://github.com/alibahri94/smart-pc)|
|2505.19564|[k-buffers: a plug-in method for enhancing neural fields with multiple buffers](https://arxiv.org/abs/2505.19564)|[k-buffers](https://github.com/renhaofan/k-buffers)|
|2505.19571|[vtbench: comprehensive benchmark suite towards real-world virtual try-on models](https://arxiv.org/abs/2505.19571)|[vtbench](https://github.com/huuxiaobin/vtbench)|
|2505.19611|[align and surpass human camouflaged perception: visual refocus reinforcement fine-tuning](https://arxiv.org/abs/2505.19611)|[vrrf](https://github.com/huuxiaobin/vrrf)|
|2505.19618|[rotation-equivariant self-supervised method in image denoising](https://arxiv.org/abs/2505.19618)|[adarenet](https://github.com/liuhanze623/adarenet)|
|2505.19638|[hf-vton: high-fidelity virtual try-on via consistent geometric and semantic alignment](https://arxiv.org/abs/2505.19638)|[hf-vton](https://github.com/mmlph/hf-vton)|
|2505.19652|[sacm: seeg-audio contrastive matching for chinese speech decoding](https://arxiv.org/abs/2505.19652)|[SACM](https://github.com/WangHongbinary/SACM)|
|2505.19659|[langdaug: langevin data augmentation for multi-source domain generalization in medical image segmentation](https://arxiv.org/abs/2505.19659)|[langdaug](https://github.com/backpropagator/langdaug)|
|2505.19692|[drivecamsim: generalizable camera simulation via explicit camera modeling for autonomous driving](https://arxiv.org/abs/2505.19692)|[drivecamsim](https://github.com/swc-17/drivecamsim)|
|2505.19742|[haodiff: human-aware one-step diffusion via dual-prompt guidance](https://arxiv.org/abs/2505.19742)|[haodiff](https://github.com/gobunu/haodiff)|
|2505.19779|[advancements in medical image classification through fine-tuning natural domain foundation models](https://arxiv.org/abs/2505.19779)|[medical-transfer-learning](https://github.com/sajjad-sh33/medical-transfer-learning)|
|2505.19793|[depth-guided bundle sampling for efficient generalizable neural radiance field reconstruction](https://arxiv.org/abs/2505.19793)|[gdb-nerf](https://github.com/klmav-cuc/gdb-nerf)|
|2505.19799|[a regularization-guided equivariant approach for image restoration](https://arxiv.org/abs/2505.19799)|[eq-reg](https://github.com/yulu919/eq-reg)|
|2505.19805|[translation-equivariance of normalization layers and aliasing in convolutional neural networks](https://arxiv.org/abs/2505.19805)|[normalization-layers](https://github.com/jscanvic/normalization-layers)|
|2505.19812|[efficient multi-modal long context learning for training-free adaptation](https://arxiv.org/abs/2505.19812)|[emloc](https://github.com/zehong-ma/emloc)|
|2505.19813|[golf-nrt: integrating global context and local geometry for few-shot view synthesis](https://arxiv.org/abs/2505.19813)|[golf-nrt](https://github.com/klmav-cuc/golf-nrt)|
|2505.19863|[fruitnerf++: a generalized multi-fruit counting method utilizing contrastive learning and neural radiance fields](https://arxiv.org/abs/2505.19863)|[fruitnerfpp](https://github.com/meyerls/fruitnerfpp)|
|2505.19877|[vad-r1: towards video anomaly reasoning via perception-to-cognition chain-of-thought](https://arxiv.org/abs/2505.19877)|[vad-r1](https://github.com/wbfwonderful/vad-r1)|
|2505.19889|[omnifall: a unified staged-to-wild benchmark for human fall detection](https://arxiv.org/abs/2505.19889)|[omnifall-experiments](https://github.com/simplexsigil/omnifall-experiments)|
|2505.19972|[phi: bridging domain shift in long-term action quality assessment via progressive hierarchical instruction](https://arxiv.org/abs/2505.19972)|[phi_aqa](https://github.com/zhoukanglei/phi_aqa)|
|2505.20024|[reasonplan: unified scene prediction and decision reasoning for closed-loop autonomous driving](https://arxiv.org/abs/2505.20024)|[reasonplan](https://github.com/liuxueyi/reasonplan)|
|2505.20038|[towards video to piano music generation with chain-of-perform support benchmarks](https://arxiv.org/abs/2505.20038)|[video-to-audio-and-piano](https://github.com/acappemin/video-to-audio-and-piano)|
|2505.20049|[data-free class-incremental gesture recognition with prototype-guided pseudo feature replay](https://arxiv.org/abs/2505.20049)|[pgpfr-3](https://github.com/sunao-101/pgpfr-3)|
|2505.20053|[multimodal llm-guided semantic correction in text-to-image diffusion](https://arxiv.org/abs/2505.20053)|[ppad](https://github.com/hellozicky/ppad)|
|2505.20107|[refining few-step text-to-multiview diffusion via reinforcement learning](https://arxiv.org/abs/2505.20107)|[mvc-zigal](https://github.com/ziyizhang27/mvc-zigal)|
|2505.20124|[tuna: comprehensive fine-grained temporal understanding evaluation on dense dynamic videos](https://arxiv.org/abs/2505.20124)|[TUNA](https://github.com/friedrichor/TUNA)|
|2505.20126|[ob3d: a new dataset for benchmarking omnidirectional 3d reconstruction using blender](https://arxiv.org/abs/2505.20126)|[omnidirectional_blender_3d_dataset](https://github.com/gsisaoki/omnidirectional_blender_3d_dataset)|
|2505.20152|[hard negative contrastive learning for fine-grained geometric understanding in large multimodal models](https://arxiv.org/abs/2505.20152)|[mmgeolm](https://github.com/thu-keg/mmgeolm)|
|2505.20156|[hunyuanvideo-avatar: high-fidelity audio-driven human animation for multiple characters](https://arxiv.org/abs/2505.20156)|[hunyuanvideo-avatar](https://github.com/tencent-hunyuan/hunyuanvideo-avatar)|
|2505.20255|[anicrafter: customizing realistic human-centric animation via avatar-background conditioning in video diffusion models](https://arxiv.org/abs/2505.20255)|[anicrafter](https://github.com/myniuuu/anicrafter)|
|2505.20256|[omni-r1: reinforcement learning for omnimodal reasoning via two-system collaboration](https://arxiv.org/abs/2505.20256)|[omni-r1](https://github.com/aim-uofa/omni-r1)|
|2505.20275|[imgedit: a unified image editing dataset and benchmark](https://arxiv.org/abs/2505.20275)|[imgedit](https://github.com/pku-yuangroup/imgedit)|
|2505.20277|[omnicharacter: towards immersive role-playing agents with seamless speech-language personality interaction](https://arxiv.org/abs/2505.20277)|[damo-convai](https://github.com/alibabaresearch/damo-convai)|
|2505.20288|[hierarchical masked autoregressive models with low-resolution token pivots](https://arxiv.org/abs/2505.20288)|[himar](https://github.com/hidream-ai/himar)|
|2505.20291|[visualized text-to-image retrieval](https://arxiv.org/abs/2505.20291)|[visualize-then-retrieve](https://github.com/xiaowu0162/visualize-then-retrieve)|
|2505.20297|[disa: diffusion step annealing in autoregressive image generation](https://arxiv.org/abs/2505.20297)|[disa](https://github.com/qinyu-allen-zhao/disa)|
|2505.20298|[mangavqa and mangalmm: a benchmark and specialized model for multimodal manga understanding](https://arxiv.org/abs/2505.20298)|[mangalmm](https://github.com/manga109/mangalmm)|
|2505.01476|[costfilter-ad: enhancing anomaly detection through matching cost filtering](https://arxiv.org/abs/2505.01476)|[costfilter-ad](https://github.com/zhe-sapi/costfilter-ad)|
|2505.05528|[x-transfer attacks: towards super transferable adversarial attacks on clip](https://arxiv.org/abs/2505.05528)|[XTransferBench](https://github.com/HanxunH/XTransferBench)|
|2505.10541|[exploring implicit visual misunderstandings in multimodal large language models through attention analysis](https://arxiv.org/abs/2505.10541)|[stme](https://github.com/welldonepf/stme)|
|2505.10595|[arfc-wahnet: adaptive receptive field convolution and wavelet-attentive hierarchical network for infrared small target detection](https://arxiv.org/abs/2505.10595)|[arfc-wahnet](https://github.com/leaf2001/arfc-wahnet)|
|2505.11454|[humanibench: a human-centric framework for large multimodal models evaluation](https://arxiv.org/abs/2505.11454)|[humanibench](https://github.com/vectorinstitute/humanibench)|
|2505.14151|[reactdiff: latent diffusion for facial reaction generation](https://arxiv.org/abs/2505.14151)|[reactdiff](https://github.com/hunan-tiger/reactdiff)|
|2505.15425|[on the robustness of medical vision-language models: are they truly generalizable?](https://arxiv.org/abs/2505.15425)|[robustmedclip](https://github.com/biomedia-mbzuai/robustmedclip)|
|2505.16809|[hypergraph tversky-aware domain incremental learning for brain tumor segmentation with missing modalities](https://arxiv.org/abs/2505.16809)|[rehydil](https://github.com/reeive/rehydil)|
|2505.16839|[lavida: a large diffusion language model for multimodal understanding](https://arxiv.org/abs/2505.16839)|[lavida](https://github.com/jacklishufan/lavida)|
|2505.16854|[think or not? selective reasoning via reinforcement learning for vision-language models](https://arxiv.org/abs/2505.16854)|[ton](https://github.com/kokolerk/ton)|
|2505.17114|[raven: query-guided representation alignment for question answering over audio, video, embedded sensors, and natural language](https://arxiv.org/abs/2505.17114)|[raven](https://github.com/bashlab/raven)|
|2505.17241|[generative ai and creativity: a systematic literature review and meta-analysis](https://arxiv.org/abs/2505.17241)|[meta-analysis-llms-creativity](https://github.com/sm2982/meta-analysis-llms-creativity)|
|2505.17440|[veattack: downstream-agnostic vision encoder attack against large vision language models](https://arxiv.org/abs/2505.17440)|[veattack-lvlm](https://github.com/hfmei/veattack-lvlm)|
|2505.17475|[posebh: prototypical multi-dataset training beyond human pose estimation](https://arxiv.org/abs/2505.17475)|[PoseBH](https://github.com/uyoung-jeong/PoseBH)|
|2505.17534|[co-reinforcement learning for unified multimodal understanding and generation](https://arxiv.org/abs/2505.17534)|[ULM-R1](https://github.com/mm-vl/ULM-R1)|
|2505.17551|[center-aware residual anomaly synthesis for multi-class industrial anomaly detection](https://arxiv.org/abs/2505.17551)|[CRAS](https://github.com/cqylunlun/CRAS)|
|2505.17556|[wildfire spread forecasting with deep learning](https://arxiv.org/abs/2505.17556)|[wildfirespread](https://github.com/orion-ai-lab/wildfirespread)|
|2505.17581|[modem: a morton-order degradation estimation mechanism for adverse weather image recovery](https://arxiv.org/abs/2505.17581)|[modem](https://github.com/hainuo-wang/modem)|
|2505.17591|[minkunext-si: improving point cloud-based place recognition including spherical coordinates and lidar intensity](https://arxiv.org/abs/2505.17591)|[minkunext-si](https://github.com/judithv/minkunext-si)|
|2505.17739|[feasible action space reduction for quantifying causal responsibility in continuous spatial interactions](https://arxiv.org/abs/2505.17739)|[continuousfear](https://github.com/dai-lab-herald/continuousfear)|
|2505.17771|[topopoint: enhance topology reasoning via endpoint detection in autonomous driving](https://arxiv.org/abs/2505.17771)|[topopoint](https://github.com/franpin/topopoint)|
|2505.17807|[temporal consistency constrained transferable adversarial attacks with background mixup for action recognition](https://arxiv.org/abs/2505.17807)|[bmtc_transferattackvid](https://github.com/mlvccn/bmtc_transferattackvid)|
|2505.17883|[fastcav: efficient computation of concept activation vectors for explaining deep neural networks](https://arxiv.org/abs/2505.17883)|[fastcav](https://gitlab.com/dlr-dw/fastcav)|
|2505.17884|[track anything annotate: video annotation and dataset generation of computer vision models](https://arxiv.org/abs/2505.17884)|[track-anything-annotate](https://github.com/lnikioffic/track-anything-annotate)|
|2505.17908|[comfymind: toward general-purpose generation via tree-based planning and reactive feedback](https://arxiv.org/abs/2505.17908)|[ComfyMind](https://github.com/EnVision-Research/ComfyMind)|
|2505.17911|[object-level cross-view geo-localization with location enhancement and multi-head cross attention](https://arxiv.org/abs/2505.17911)|[ocgnet](https://github.com/zheyangh/ocgnet)|
|2505.18015|[semsegbench & detecbench: benchmarking reliability and generalization beyond classification](https://arxiv.org/abs/2505.18015)|[benchmarking_reliability_generalization](https://github.com/shashankskagnihotri/benchmarking_reliability_generalization)|
|2505.18025|[3d face reconstruction error decomposed: a modular benchmark for fair and fast method evaluation](https://arxiv.org/abs/2505.18025)|[M3DFB](https://github.com/sariyanidi/M3DFB)|
|2505.18028|[knot so simple: a minimalistic environment for spatial reasoning](https://arxiv.org/abs/2505.18028)|[knotgym](https://github.com/lil-lab/knotgym)|
|2505.18035|[camme: adaptive deepfake image detection with multi-modal cross-attention](https://arxiv.org/abs/2505.18035)|[camme](https://github.com/magnet300/camme)|
|2505.18153|[ren: fast and efficient region encodings from patch-based image encoders](https://arxiv.org/abs/2505.18153)|[ren](https://github.com/savya08/ren)|


## Archives
- [May 2025](archives/2025/05.md)
- [April 2025](archives/2025/04.md)
